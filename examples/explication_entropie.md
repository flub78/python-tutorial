# Explication dÃ©taillÃ©e du calcul de l'entropie

## Vue d'ensemble

La fonction `calculerEntropie(motTest, candidats)` mesure **l'incertitude** ou la **quantitÃ© d'information** qu'apporte un mot test. Plus l'entropie est Ã©levÃ©e, plus le mot est efficace pour rÃ©duire l'espace de recherche.

## ğŸ¯ Objectif

Trouver le mot qui divise le mieux l'ensemble des candidats possibles, c'est-Ã -dire celui qui permet d'Ã©liminer le plus de possibilitÃ©s, quelle que soit la rÃ©ponse obtenue.

## ğŸ“Š Ã‰tapes du calcul

### **1. Partitionnement**

```javascript
let partitions = {};

candidats.forEach(candidat => {
    let pattern = genererPattern(motTest, candidat);
    if (!partitions[pattern]) {
        partitions[pattern] = [];
    }
    partitions[pattern].push(candidat);
});
```

**Ce qui se passe :**
- Pour chaque candidat possible, on calcule quel serait le pattern de rÃ©ponse si ce candidat Ã©tait le mot secret
- Les candidats sont regroupÃ©s par pattern identique
- Exemple : si `motTest = "SALER"` et qu'on teste contre les mots `SALON`, `SAPIN`, `SACHE`...
  - `SALON` â†’ pattern `"==_?_"` (S bien placÃ©, A bien placÃ©, L absent, O mal placÃ©, N absent)
  - `SAPIN` â†’ pattern `"==___"` 
  - Les mots avec le mÃªme pattern sont regroupÃ©s ensemble

**Structure des partitions :**
```javascript
partitions = {
    "==_?_": ["SALON", "SABOT"],  // 2 mots
    "==___": ["SAPIN", "SACHE"],  // 2 mots
    "===__": ["SALER"],           // 1 mot
    // etc.
}
```

### **2. Calcul de l'entropie de Shannon**

```javascript
let entropie = 0;
let n = candidats.length;  // Nombre total de candidats

Object.values(partitions).forEach(groupe => {
    let p = groupe.length / n;  // ProbabilitÃ© de ce pattern
    if (p > 0) {
        entropie -= p * Math.log2(p);
    }
});
```

**Formule mathÃ©matique :**

$$H = -\sum_{i=1}^{k} p_i \log_2(p_i)$$

OÃ¹ :
- $H$ = entropie (en bits)
- $p_i$ = probabilitÃ© du pattern $i$ (= nombre de mots dans le groupe / nombre total de candidats)
- $k$ = nombre de patterns diffÃ©rents

**Exemple concret :**

Supposons 8 candidats rÃ©partis ainsi :
- Pattern A : 4 mots â†’ $p_A = 4/8 = 0.5$
- Pattern B : 2 mots â†’ $p_B = 2/8 = 0.25$
- Pattern C : 2 mots â†’ $p_C = 2/8 = 0.25$

Calcul :
```
H = -(0.5 Ã— logâ‚‚(0.5) + 0.25 Ã— logâ‚‚(0.25) + 0.25 Ã— logâ‚‚(0.25))
H = -(0.5 Ã— (-1) + 0.25 Ã— (-2) + 0.25 Ã— (-2))
H = -(-0.5 - 0.5 - 0.5)
H = 1.5 bits
```

## ğŸ’¡ InterprÃ©tation

### Entropie Ã©levÃ©e (bon mot) âœ…

- Les candidats sont rÃ©partis uniformÃ©ment dans beaucoup de partitions
- Exemple : 8 candidats â†’ 8 patterns diffÃ©rents (1 mot par pattern) â†’ $H = 3$ bits
- **Le mot divise efficacement l'espace de recherche**

### Entropie faible (mauvais mot) âŒ

- Les candidats sont concentrÃ©s dans peu de partitions
- Exemple : 8 candidats â†’ 1 seul pattern (tous ensemble) â†’ $H = 0$ bits
- **Le mot n'apporte presque aucune information**

## ğŸ”„ Utilisation dans le solveur

La fonction `meilleurMot()` teste tous les candidats et choisit celui avec l'**entropie maximale** :

```javascript
function meilleurMot(candidats) {
    if (candidats.length === 1) {
        return candidats[0];
    }

    let meilleur = null;
    let maxEntropie = -1;

    candidats.forEach(mot => {
        let entropie = calculerEntropie(mot, candidats);
        if (entropie > maxEntropie) {
            maxEntropie = entropie;
            meilleur = mot;
        }
    });

    return meilleur;
}
```

## ğŸ“š Fondements thÃ©oriques

C'est une stratÃ©gie **optimale** basÃ©e sur la **thÃ©orie de l'information de Claude Shannon**, qui garantit de trouver le mot en un minimum d'essais en moyenne.

### Avantages de cette approche

1. **Optimale en moyenne** : Minimise le nombre d'essais attendu
2. **BasÃ©e sur la thÃ©orie de l'information** : Solide fondement mathÃ©matique
3. **PrÃ©dictive** : Anticipe toutes les rÃ©ponses possibles
4. **Ã‰quitable** : Ne favorise aucun pattern particulier

### ComplexitÃ© algorithmique

- Pour $n$ candidats et $m$ mots Ã  tester
- ComplexitÃ© : $O(m \times n)$ pour trouver le meilleur mot
- En pratique, $m = n$ (on teste tous les candidats)
- Donc : $O(n^2)$ par itÃ©ration

## ğŸ® Application pratique dans Sutom

1. **DÃ©marrage** : L'algorithme charge tous les mots correspondant aux critÃ¨res (premiÃ¨re lettre + longueur)
2. **PremiÃ¨re proposition** : Calcule l'entropie de chaque mot candidat et propose celui avec la plus haute entropie
3. **AprÃ¨s chaque rÃ©ponse** : 
   - Filtre les candidats selon le pattern obtenu
   - Recalcule l'entropie sur les candidats restants
   - Propose le nouveau meilleur mot
4. **Convergence** : Continue jusqu'Ã  trouver le mot ou n'avoir plus qu'un candidat

## ğŸ“Š Exemple de progression

```
Ã‰tape 1: 1000 candidats â†’ Entropie max = 8.5 bits â†’ Propose "SALER"
RÃ©ponse: "==_?_" â†’ 50 candidats restants

Ã‰tape 2: 50 candidats â†’ Entropie max = 4.2 bits â†’ Propose "SAINT"
RÃ©ponse: "===__" â†’ 3 candidats restants

Ã‰tape 3: 3 candidats â†’ Entropie max = 1.5 bits â†’ Propose "SABOT"
RÃ©ponse: "=====" â†’ Mot trouvÃ© !
```

Chaque Ã©tape rÃ©duit drastiquement l'espace de recherche grÃ¢ce Ã  la maximisation de l'information obtenue.
